{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6112dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb303f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['intent', 'user_utterance', 'origin'],\n",
      "    num_rows: 600000\n",
      "})\n",
      "Features: {'intent': Value('string'), 'user_utterance': Value('string'), 'origin': Value('string')}\n",
      "First example: {'intent': 'accept_reservations', 'user_utterance': 'am i able to make reservations at spago in beverly hills', 'origin': 'original'}\n"
     ]
    }
   ],
   "source": [
    "# Load the CLINIC150-sur dataset from Hugging Face\n",
    "full_dataset = load_dataset(\"ibm-research/clinic150-sur\", split=\"train\")  # 'train' is the only split and contains all data\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(full_dataset)\n",
    "print(\"Features:\", full_dataset.features)\n",
    "print(\"First example:\", full_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intent mapping\n",
    "# We map a subset of the original intents to our custom intents\n",
    "INTENT_MAP = {\n",
    "    # \"avail\" -> check availability\n",
    "    \"calendar\": \"avail\",\n",
    "    \"how_busy\": \"avail\",\n",
    "    \"date\": \"avail\",\n",
    "    \"meeting_schedule\": \"avail\",\n",
    "    \n",
    "\t# \"book\" -> book an appointment\n",
    "    \"accept_reservation\": \"book\",\n",
    "    \"schedule_meeting\": \"book\",\n",
    "    \"yes\": \"book\",\n",
    "    \n",
    "\t# \"bye\" -> end conversation\n",
    "    \"goodbye\": \"bye\",\n",
    "    \"thank_you\": \"bye\",\n",
    "    \n",
    "\t# \"cancel\" -> cancel an appointment\n",
    "    \"cancel_reservation\": \"cancel\",\n",
    "    \"no\": \"cancel\",\n",
    "    \n",
    "\t# \"greet\" -> start conversation\n",
    "    \"greeting\": \"greet\",\n",
    "    \n",
    "\t# \"resched\" -> reschedule an appointment\n",
    "    \"calendar_update\": \"resched\",\n",
    "    \"reminder_update\": \"resched\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a669e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 97891\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to keep only examples whose intent is in INTENT_MAP\n",
    "filtered_dataset = full_dataset.filter(\n",
    "\tlambda example: example['intent'] in INTENT_MAP\n",
    ")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe1f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: {'intent': Value('string'), 'user_utterance': Value('string'), 'origin': Value('string'), 'new_intent': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# Apply the INTENT_MAP to create a new column 'new_intent' in filtered_dataset\n",
    "def map_intent(example):\n",
    "\treturn {'new_intent': INTENT_MAP[example['intent']]}\n",
    "\n",
    "processed_dataset = filtered_dataset.map(map_intent)\n",
    "print(\"Features:\", processed_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b819e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: {'intent': Value('string'), 'user_utterance': Value('string'), 'origin': Value('string'), 'new_intent': Value('string'), 'label': ClassLabel(names=['avail', 'book', 'bye', 'cancel', 'greet', 'resched'])}\n",
      "First example: {'intent': 'calendar', 'user_utterance': 'anything on the schedule for october 14th', 'origin': 'original', 'new_intent': 'avail', 'label': 0}\n",
      "New intents and their IDs: {'avail': 0, 'book': 1, 'bye': 2, 'cancel': 3, 'greet': 4, 'resched': 5}\n"
     ]
    }
   ],
   "source": [
    "# Get unique new_intent values and sort them for consistent label assignment\n",
    "unique_intents = sorted(set(processed_dataset['new_intent']))\n",
    "new_intent_to_id = {intent: idx for idx, intent in enumerate(unique_intents)}\n",
    "\n",
    "# Create a ClassLabel feature\n",
    "class_label = ClassLabel(names=unique_intents)\n",
    "\n",
    "# Add 'label' column by mapping 'new_intent' to its integer id\n",
    "def encode_label(example):\n",
    "\treturn {'label': new_intent_to_id[example['new_intent']]}\n",
    "\n",
    "final_dataset = processed_dataset.map(encode_label)\n",
    "final_dataset = final_dataset.cast_column('label', class_label)\n",
    "print(\"Features:\", final_dataset.features)\n",
    "print(\"First example:\", final_dataset[0])\n",
    "print(\"New intents and their IDs:\", new_intent_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15da8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the dataset into training and testing sets (80/20 split)\n",
    "# First split: 15% test, 85% train+val\n",
    "split_1 = final_dataset.train_test_split(test_size=0.15, stratify_by_column='label', seed=42)\n",
    "\n",
    "# Second split: from train+val, 15/85 â‰ˆ 0.176 for validation (so val is 15% of total)\n",
    "split_2 = split_1['train'].train_test_split(test_size=0.176, stratify_by_column='label', seed=42)\n",
    "\n",
    "# Assemble splits\n",
    "data_split = {\n",
    "    'train': split_2['train'],\n",
    "    'val': split_2['test'],\n",
    "    'test': split_1['test']\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame for easy saving to CSV\n",
    "train_df = data_split['train'].to_pandas()\n",
    "val_df = data_split['val'].to_pandas()\n",
    "test_df = data_split['test'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11b17c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(df, label_col):\n",
    "    \"\"\"\n",
    "    Print the class distribution for a given label column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        label_col (str): The name of the column containing class labels.\n",
    "\n",
    "    Prints:\n",
    "        A DataFrame showing the count and percentage of each class.\n",
    "    \"\"\"\n",
    "    # Count the occurrences of each class\n",
    "    counts = df[label_col].value_counts(dropna=False)\n",
    "    # Calculate the percentage of each class\n",
    "    percentages = df[label_col].value_counts(normalize=True, dropna=False) * 100\n",
    "    # Display the results as a DataFrame\n",
    "    print(pd.DataFrame({'count': counts, 'percent': percentages.round(2)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d76bc461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 68562, Val: 14645, Test: 14684\n",
      "\n",
      "Train set class distribution:\n",
      "            count  percent\n",
      "new_intent                \n",
      "bye         18278    26.66\n",
      "book        13938    20.33\n",
      "cancel      11804    17.22\n",
      "avail       11514    16.79\n",
      "greet       10180    14.85\n",
      "resched      2848     4.15\n",
      "\n",
      "Validation set class distribution:\n",
      "            count  percent\n",
      "new_intent                \n",
      "bye          3904    26.66\n",
      "book         2977    20.33\n",
      "cancel       2521    17.21\n",
      "avail        2459    16.79\n",
      "greet        2175    14.85\n",
      "resched       609     4.16\n",
      "\n",
      "Test set class distribution:\n",
      "            count  percent\n",
      "new_intent                \n",
      "bye          3915    26.66\n",
      "book         2985    20.33\n",
      "cancel       2528    17.22\n",
      "avail        2466    16.79\n",
      "greet        2180    14.85\n",
      "resched       610     4.15\n"
     ]
    }
   ],
   "source": [
    "# Disolay the sizes of the splits\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Display class distributions for each split\n",
    "print(f\"\\nTrain set class distribution:\")\n",
    "print_class_distribution(train_df, 'new_intent')\n",
    "print(f\"\\nValidation set class distribution:\")\n",
    "print_class_distribution(val_df, 'new_intent')\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print_class_distribution(test_df, 'new_intent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d38e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to C:\\Users\\aceto\\Documents\\GitHub\\schedulebot-plus\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Get the project root (parent of the current notebook)\n",
    "project_root = Path().cwd().resolve().parent\n",
    "output_dir = project_root / \"data\" / \"processed\"\n",
    "\n",
    "# Save the files\n",
    "train_df.to_csv(output_dir / \"train.csv\", index=False)\n",
    "val_df.to_csv(output_dir / \"val.csv\", index=False)\n",
    "test_df.to_csv(output_dir / \"test.csv\", index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedulebot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
