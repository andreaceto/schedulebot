{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from seqeval.metrics import f1_score as ner_f1_score\n",
    "from seqeval.scheme import IOB2 # For entity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom model architecture\n",
    "# Build an absolute path from this notebook's parent directory\n",
    "module_path = os.path.abspath(os.path.join('..', 'src', 'schedulebot', 'nlu'))\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Now you can import the desired function or class\n",
    "from multitask_model import MultitaskModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e47f6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Login and Configuration ---\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "hub_model_id = os.getenv(\"HUB_MODEL_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Tokenizer ---\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e02085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Processed Dataset ---\n",
    "project_root = str(Path().cwd().resolve().parent)\n",
    "dataset_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "processed_datasets = load_from_disk(os.path.join(dataset_dir, \"hasd_processed\"))\n",
    "print(processed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc068c7a",
   "metadata": {},
   "source": [
    "## Custom Metrics Function\n",
    "This function is essential for a multitask model. It will be called by the `Trainer` at the end of each epoch to calculate both intent accuracy and NER F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Unpack predictions and labels\n",
    "    predictions, label_values = eval_pred\n",
    "    intent_preds, ner_preds = predictions\n",
    "    intent_labels, ner_labels = label_values\n",
    "\n",
    "    # --- Intent Metrics ---\n",
    "    intent_preds = np.argmax(intent_preds, axis=1)\n",
    "    intent_accuracy = accuracy_score(intent_labels, intent_preds)\n",
    "    intent_f1 = f1_score(intent_labels, intent_preds, average='weighted')\n",
    "\n",
    "    # --- NER Metrics ---\n",
    "    ner_preds = np.argmax(ner_preds, axis=2)\n",
    "\n",
    "    # Remove padding tokens (where label is -100) and convert IDs to labels\n",
    "    true_ner_labels = []\n",
    "    true_ner_predictions = []\n",
    "    id2ner = processed_datasets['train'].features['labels'].feature.names\n",
    "\n",
    "    for i in range(len(ner_labels)):\n",
    "        true_labels_row = []\n",
    "        true_predictions_row = []\n",
    "        for j in range(len(ner_labels[i])):\n",
    "            if ner_labels[i][j] != -100:\n",
    "                true_labels_row.append(id2ner[ner_labels[i][j]])\n",
    "                true_predictions_row.append(id2ner[ner_preds[i][j]])\n",
    "        true_ner_labels.append(true_labels_row)\n",
    "        true_ner_predictions.append(true_predictions_row)\n",
    "\n",
    "    ner_f1 = ner_f1_score(true_ner_labels, true_ner_predictions, mode='strict', scheme=IOB2)\n",
    "\n",
    "    return {\n",
    "        \"intent_accuracy\": intent_accuracy,\n",
    "        \"intent_f1\": intent_f1,\n",
    "        \"ner_f1\": ner_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd99f4",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "We now create an instance of our `MultitaskModel`, passing it a configuration object that includes the number of labels for each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e98e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Label Mappings Directly from Data ---\n",
    "\n",
    "# 1. Intent Labels\n",
    "# Get a sorted list of unique intent strings from the training set\n",
    "intent_label_list = processed_datasets['train'].features['intent_label'].names\n",
    "# Create the mappings\n",
    "id2intent = {i: label for i, label in enumerate(intent_label_list)}\n",
    "intent2id = {label: i for i, label in enumerate(intent_label_list)}\n",
    "\n",
    "print(\"--- Intent Vocab ---\")\n",
    "print(f\"Number of intents: {len(id2intent)}\")\n",
    "print(f\"Mapping (id2intent): {id2intent}\")\n",
    "print(f\"Mapping (intent2id): {intent2id}\\n\")\n",
    "\n",
    "\n",
    "# 2. NER Labels\n",
    "# Get a sorted list of unique entities strings from the training set\n",
    "ner_label_list = processed_datasets['train'].features['labels'].feature.names\n",
    "# Create the mappings\n",
    "id2ner = {i: label for i, label in enumerate(ner_label_list)}\n",
    "ner2id = {label: i for i, label in enumerate(ner_label_list)}\n",
    "\n",
    "print(\"--- NER Vocab ---\")\n",
    "print(f\"Number of NER tags: {len(id2ner)}\")\n",
    "print(f\"Mapping (id2ner): {id2ner}\")\n",
    "print(f\"Mapping (ner2id): {ner2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Add custom parameters to config object\n",
    "# This ensures they are saved in config.json on the Hub\n",
    "config.num_intent_labels = len(id2intent)\n",
    "config.num_ner_labels = len(id2ner)\n",
    "config.id2label_intent = id2intent\n",
    "config.label2id_intent = intent2id\n",
    "config.id2label_ner = id2ner\n",
    "config.label2id_ner = ner2id\n",
    "\n",
    "model = MultitaskModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb8c8b",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we define the `TrainingArguments` and create a `Trainer` instance to handle the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FREEZE THE BASE MODEL ---\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator to handle padding for token classification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(project_root, \"models\", \"multitask_model\", \"training\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=200,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # --- Hub Arguments ---\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"end\",\n",
    "    hub_token=hf_token,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "train_df = df[df[\"loss\"].notna()][['epoch', 'loss']]\n",
    "eval_df = df[df[\"eval_loss\"].notna()][['epoch', 'eval_loss']]\n",
    "\n",
    "# --- Plot the loss curves ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot training loss\n",
    "ax.plot(train_df[\"epoch\"], train_df[\"loss\"], label=\"Training Loss\", color=\"dodgerblue\")\n",
    "\n",
    "# Plot validation loss\n",
    "ax.plot(eval_df[\"epoch\"], eval_df[\"eval_loss\"], label=\"Validation Loss\", color=\"darkorange\", linestyle='--')\n",
    "\n",
    "# --- 4. Customize and show the plot ---\n",
    "ax.set_title(\"Training & Validation Loss Curves\", fontsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=12)\n",
    "ax.set_ylabel(\"Loss\", fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93425e",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e95635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UNFREEZE THE BASE MODEL ---\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments for the fine-tuning stage\n",
    "training_args_stage2 = TrainingArguments(\n",
    "    output_dir=os.path.join(project_root, \"models\", \"multitask_model\", \"finetuning\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=1e-3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # --- Hub Arguments ---\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"end\",\n",
    "    hub_token=hf_token,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Create a new Trainer for Stage 2\n",
    "trainer_stage2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_stage2,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stage2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c285033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(trainer_stage2.state.log_history)\n",
    "\n",
    "train_df = df[df[\"loss\"].notna()][['epoch', 'loss']]\n",
    "eval_df = df[df[\"eval_loss\"].notna()][['epoch', 'eval_loss']]\n",
    "\n",
    "# --- Plot the loss curves ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot training loss\n",
    "ax.plot(train_df[\"epoch\"], train_df[\"loss\"], label=\"Training Loss\", color=\"dodgerblue\")\n",
    "\n",
    "# Plot validation loss\n",
    "ax.plot(eval_df[\"epoch\"], eval_df[\"eval_loss\"], label=\"Validation Loss\", color=\"darkorange\", linestyle='--')\n",
    "\n",
    "# --- 4. Customize and show the plot ---\n",
    "ax.set_title(\"Training & Validation Loss Curves\", fontsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=12)\n",
    "ax.set_ylabel(\"Loss\", fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ddddd",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8dcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the trainer\n",
    "test_predictions = trainer_stage2.predict(processed_datasets[\"test\"])\n",
    "\n",
    "# Unpack the predictions and true labels\n",
    "intent_preds_logits, ner_preds_logits = test_predictions.predictions\n",
    "intent_true_labels, ner_true_labels = test_predictions.label_ids\n",
    "\n",
    "# Get the final predicted class IDs by finding the max logit\n",
    "intent_pred_labels = np.argmax(intent_preds_logits, axis=1)\n",
    "ner_pred_labels = np.argmax(ner_preds_logits, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Intent Classification Report and Confusion Matrix ---\n",
    "\n",
    "print(\"\\n--- Intent Classification Report ---\")\n",
    "\n",
    "# Get the human-readable intent names from the dataset features\n",
    "intent_names = processed_datasets['test'].features['intent_label'].names\n",
    "\n",
    "# Generate and print the classification report\n",
    "intent_report = classification_report(intent_true_labels, intent_pred_labels, target_names=intent_names, digits=4)\n",
    "print(intent_report)\n",
    "\n",
    "# Generate and plot the confusion matrix\n",
    "intent_cm = confusion_matrix(intent_true_labels, intent_pred_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(intent_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=intent_names, yticklabels=intent_names)\n",
    "plt.xlabel('Predicted Intent')\n",
    "plt.ylabel('True Intent')\n",
    "plt.title('Intent Classification Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9977a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NER Report and Confusion Matrix ---\n",
    "\n",
    "print(\"\\n--- NER (Token Classification) Report ---\")\n",
    "\n",
    "# Get the human-readable NER tag names\n",
    "ner_tag_names = processed_datasets['test'].features['labels'].feature.names\n",
    "\n",
    "# Flatten the lists of predictions and labels, ignoring -100 tokens\n",
    "true_ner_tags_flat = []\n",
    "pred_ner_tags_flat = []\n",
    "\n",
    "for i in range(len(ner_true_labels)):\n",
    "    for j in range(len(ner_true_labels[i])):\n",
    "        if ner_true_labels[i][j] != -100: # Ignore padding\n",
    "            true_ner_tags_flat.append(ner_tag_names[ner_true_labels[i][j]])\n",
    "            pred_ner_tags_flat.append(ner_tag_names[ner_pred_labels[i][j]])\n",
    "\n",
    "# Generate and print the classification report for NER\n",
    "ner_report = classification_report(true_ner_tags_flat, pred_ner_tags_flat, digits=4)\n",
    "print(ner_report)\n",
    "\n",
    "# Generate and plot the confusion matrix for NER\n",
    "# Note: This matrix can be large if you have many entity types.\n",
    "ner_cm = confusion_matrix(true_ner_tags_flat, pred_ner_tags_flat, labels=ner_tag_names)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(ner_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=ner_tag_names, yticklabels=ner_tag_names)\n",
    "plt.xlabel('Predicted NER Tag')\n",
    "plt.ylabel('True NER Tag')\n",
    "plt.title('NER (Token) Confusion Matrix')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Pushing final model to Hub ---\")\n",
    "trainer_stage2.push_to_hub()\n",
    "print(f\"âœ… Model successfully pushed to https://huggingface.co/{hub_model_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedulebot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
