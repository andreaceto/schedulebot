{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64805fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48980dcd",
   "metadata": {},
   "source": [
    "## Load all CSV files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3adef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = str(Path().cwd().resolve().parent)\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "entities_dir = os.path.join(raw_data_dir, \"entities\")\n",
    "intents_dir = os.path.join(raw_data_dir, \"intents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4de5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 practitioners.\n",
      "Loaded 147 appointment types.\n",
      "Loaded 150 pre-generated appointment IDs.\n",
      "Loaded 'schedule.csv' with 139 rows.\n",
      "Loaded 'reschedule.csv' with 143 rows.\n",
      "Loaded 'cancel.csv' with 142 rows.\n",
      "Loaded 'query_avail.csv' with 140 rows.\n",
      "Loaded 'greeting.csv' with 150 rows.\n",
      "Loaded 'bye.csv' with 150 rows.\n",
      "Loaded 'positive_reply.csv' with 150 rows.\n",
      "Loaded 'negative_reply.csv' with 150 rows.\n",
      "Loaded 'oos.csv' with 150 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Entity Value Lists ---\n",
    "try:\n",
    "    practitioner_df = pd.read_csv(os.path.join(entities_dir, \"practitioner_name.csv\"))\n",
    "    practitioner_list = practitioner_df['text'].tolist()\n",
    "\n",
    "    appointment_type_df = pd.read_csv(os.path.join(entities_dir, \"appointment_type.csv\"))\n",
    "    appointment_type_list = appointment_type_df['text'].tolist()\n",
    "    \n",
    "    # Load the pre-generated appointment IDs\n",
    "    appointment_id_df = pd.read_csv(os.path.join(entities_dir, \"appointment_id.csv\"))\n",
    "    appointment_id_list = appointment_id_df['text'].tolist()\n",
    "    \n",
    "    print(f\"Loaded {len(practitioner_list)} practitioners.\")\n",
    "    print(f\"Loaded {len(appointment_type_list)} appointment types.\")\n",
    "    print(f\"Loaded {len(appointment_id_list)} pre-generated appointment IDs.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading entity CSVs: {e}\")\n",
    "    print(\"Please ensure you have run id_generator.py and all entity files are in place.\")\n",
    "\n",
    "\n",
    "# --- Load Intent Template DataFrames ---\n",
    "intent_dfs = {}\n",
    "intent_files = [\n",
    "    \"schedule\", \"reschedule\", \"cancel\", \"query_avail\",\n",
    "    \"greeting\", \"bye\", \"positive_reply\", \"negative_reply\", \"oos\"\n",
    "]\n",
    "\n",
    "for intent_name in intent_files:\n",
    "    try:\n",
    "        path = os.path.join(intents_dir, f\"{intent_name}.csv\")\n",
    "        intent_dfs[intent_name] = pd.read_csv(path)\n",
    "        print(f\"Loaded '{intent_name}.csv' with {len(intent_dfs[intent_name])} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find '{intent_name}.csv'. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54591b",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "This is the new, core section. We define a function, `augment_templates`, that uses a pre-trained language model to create new sentence variations.\n",
    "\n",
    "- How it works:\n",
    "\n",
    "\t1. It initializes a `fill-mask` pipeline from `transformers`, which is good at predicting words in context.\n",
    "\t2. The function iterates through each word of a template sentence.\n",
    "\t3. It skips placeholders (like `{practitioner_name}`) to ensure they are preserved.\n",
    "\t4. It replaces one word at a time with a `[MASK]` token.\n",
    "\t5. It asks the language model to predict the best words to fill that mask.\n",
    "\t6. It creates a new template using a suitable prediction, effectively paraphrasing the original.\n",
    "\n",
    "We then apply this function to the templates for our complex intents (`schedule`, `cancel`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e34ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Augmenting Complex Intents ---\n",
      "Augmenting intent: 'schedule'...\n",
      "  -> Original: 139 templates | Augmented: 534 templates\n",
      "Augmenting intent: 'reschedule'...\n",
      "  -> Original: 143 templates | Augmented: 554 templates\n",
      "Augmenting intent: 'cancel'...\n",
      "  -> Original: 142 templates | Augmented: 548 templates\n",
      "Augmenting intent: 'query_avail'...\n",
      "  -> Original: 140 templates | Augmented: 536 templates\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Augmentation Pipeline ---\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "\n",
    "# --- Define Placeholders and Cleanup Function ---\n",
    "known_placeholders = [\"{practitioner_name}\", \"{appointment_type}\", \"{appointment_id}\"]\n",
    "\n",
    "def cleanup_template(text):\n",
    "    \"\"\"\n",
    "    Sanitizes an augmented template to fix corrupted placeholders.\n",
    "    \"\"\"\n",
    "    # 1. Fix missing closing braces\n",
    "    for ph in known_placeholders:\n",
    "        # Find instances like \"{practitioner_name\" (missing the closing brace)\n",
    "        corrupted_ph = ph[:-1] # e.g., \"{practitioner_name\"\n",
    "        if corrupted_ph in text and ph not in text:\n",
    "            text = text.replace(corrupted_ph, ph)\n",
    "            \n",
    "    # 2. Remove any stray opening braces that are not part of a known placeholder\n",
    "    # This finds any \"{\" that is not followed by a known entity name\n",
    "    text = re.sub(r'\\{(?!practitioner_name|appointment_type|appointment_id)', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def augment_templates(templates, num_augmentations_per_template=3):\n",
    "    \"\"\"\n",
    "    Augments a list of templates using contextual word replacement\n",
    "    and cleans up any resulting placeholder corruption.\n",
    "    \"\"\"\n",
    "    augmented_templates = list(templates) # Start with all original templates\n",
    "    \n",
    "    for template in templates:\n",
    "        # This regex splits the text by spaces while keeping placeholders intact\n",
    "        words_and_placeholders = re.findall(r'\\{[a-zA-Z_]+\\}|\\S+', template)\n",
    "        \n",
    "        potential_indices = [\n",
    "            i for i, item in enumerate(words_and_placeholders) \n",
    "            if item not in known_placeholders\n",
    "        ]\n",
    "        \n",
    "        if not potential_indices:\n",
    "            continue\n",
    "            \n",
    "        for _ in range(num_augmentations_per_template):\n",
    "            idx_to_replace = random.choice(potential_indices)\n",
    "            original_word = words_and_placeholders[idx_to_replace]\n",
    "            \n",
    "            temp_list = words_and_placeholders.copy()\n",
    "            temp_list[idx_to_replace] = unmasker.tokenizer.mask_token\n",
    "            masked_text = ' '.join(temp_list)\n",
    "            \n",
    "            predictions = [\n",
    "                pred['token_str'] for pred in unmasker(masked_text, top_k=5)\n",
    "                if '##' not in pred['token_str'] and pred['token_str'].lower() != original_word.lower()\n",
    "            ]\n",
    "            \n",
    "            if predictions:\n",
    "                new_word = random.choice(predictions)\n",
    "                new_template_list = words_and_placeholders.copy()\n",
    "                new_template_list[idx_to_replace] = new_word\n",
    "                \n",
    "                # Create the augmented sentence\n",
    "                augmented_sentence = ' '.join(new_template_list)\n",
    "                \n",
    "                # **Apply the cleanup function**\n",
    "                cleaned_sentence = cleanup_template(augmented_sentence)\n",
    "                \n",
    "                augmented_templates.append(cleaned_sentence)\n",
    "                    \n",
    "    return list(set(augmented_templates))\n",
    "\n",
    "# --- Apply Augmentation to Complex Intents ---\n",
    "print(\"\\\\n--- Augmenting Complex Intents ---\")\n",
    "complex_intents = [\"schedule\", \"reschedule\", \"cancel\", \"query_avail\"]\n",
    "\n",
    "for intent in complex_intents:\n",
    "    if intent in intent_dfs:\n",
    "        print(f\"Augmenting intent: '{intent}'...\")\n",
    "        original_df = intent_dfs[intent]\n",
    "        original_templates = original_df['text'].tolist()\n",
    "        \n",
    "        augmented_templates = augment_templates(original_templates)\n",
    "        \n",
    "        augmented_df = pd.DataFrame(augmented_templates, columns=['text'])\n",
    "        augmented_df['intent'] = intent\n",
    "        \n",
    "        intent_dfs[intent] = augmented_df\n",
    "        print(f\"  -> Original: {len(original_templates)} templates | Augmented: {len(augmented_df)} templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac207476",
   "metadata": {},
   "source": [
    "## Process Intents and Inject Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6164c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 750 entries from simple intents.\n",
      "Processing complete. Total entries in dataset: 2922.\n"
     ]
    }
   ],
   "source": [
    "final_dataset = []\n",
    "\n",
    "# --- Process Simple Intents ---\n",
    "simple_intents = [\"greeting\", \"bye\", \"positive_reply\", \"negative_reply\", \"oos\"]\n",
    "for intent_name in simple_intents:\n",
    "    if intent_name in intent_dfs:\n",
    "        for index, row in intent_dfs[intent_name].iterrows():\n",
    "            final_dataset.append({\"text\": row['text'], \"intent\": row['intent'], \"entities\": []})\n",
    "print(f\"Processed {len(final_dataset)} entries from simple intents.\")\n",
    "\n",
    "# --- Process Complex Intents ---\n",
    "entity_map = {\n",
    "    'practitioner_name': practitioner_list,\n",
    "    'appointment_type': appointment_type_list,\n",
    "    'appointment_id': appointment_id_list\n",
    "}\n",
    "\n",
    "for intent_name in complex_intents:\n",
    "    if intent_name in intent_dfs:\n",
    "        for index, row in intent_dfs[intent_name].iterrows():\n",
    "            template = row['text']\n",
    "            injected_text = template\n",
    "            entities = []\n",
    "            placeholders = re.findall(r\"\\{(.+?)\\}\", template)\n",
    "            \n",
    "            for placeholder in placeholders:\n",
    "                # The logic is now generalized for all entities\n",
    "                if placeholder in entity_map:\n",
    "                    value_to_inject = random.choice(entity_map[placeholder])\n",
    "                    start_index = injected_text.find(\"{\" + placeholder + \"}\")\n",
    "                    if start_index != -1:\n",
    "                        injected_text = injected_text.replace(\"{\" + placeholder + \"}\", value_to_inject, 1)\n",
    "                        end_index = start_index + len(value_to_inject)\n",
    "                        entities.append({\"start\": start_index, \"end\": end_index, \"label\": placeholder})\n",
    "                else:\n",
    "                    print(f\"Warning: Found unknown placeholder '{placeholder}'\")\n",
    "\n",
    "            final_dataset.append({\n",
    "                \"text\": injected_text,\n",
    "                \"intent\": row['intent'],\n",
    "                \"entities\": sorted(entities, key=lambda e: e['start'])\n",
    "            })\n",
    "print(f\"Processing complete. Total entries in dataset: {len(final_dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6229d",
   "metadata": {},
   "source": [
    "## Shuffle and Save the final JSONL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b96d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shuffled.\n",
      "✅ Success! Dataset correctly saved.\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(final_dataset)\n",
    "print(\"Dataset shuffled.\")\n",
    "\n",
    "output_path_jsonl = os.path.join(raw_data_dir, \"hasd.jsonl\")\n",
    "with open(output_path_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for entry in final_dataset:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✅ Success! Dataset correctly saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedulebot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
