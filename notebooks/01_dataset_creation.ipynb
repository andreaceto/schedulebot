{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64805fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48980dcd",
   "metadata": {},
   "source": [
    "## Load all CSV files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3adef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = str(Path().cwd().resolve().parent)\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "entities_dir = os.path.join(raw_data_dir, \"entities\")\n",
    "intents_dir = os.path.join(raw_data_dir, \"intents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4de5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 practitioners.\n",
      "Loaded 147 appointment types.\n",
      "Loaded 1500 pre-generated appointment IDs.\n",
      "Loaded 'schedule.csv' with 139 rows.\n",
      "Loaded 'reschedule.csv' with 143 rows.\n",
      "Loaded 'cancel.csv' with 142 rows.\n",
      "Loaded 'query_avail.csv' with 140 rows.\n",
      "Loaded 'greeting.csv' with 150 rows.\n",
      "Loaded 'bye.csv' with 150 rows.\n",
      "Loaded 'positive_reply.csv' with 300 rows.\n",
      "Loaded 'negative_reply.csv' with 150 rows.\n",
      "Loaded 'oos.csv' with 1350 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Entity Value Lists ---\n",
    "try:\n",
    "    practitioner_df = pd.read_csv(os.path.join(entities_dir, \"practitioner_name.csv\"))\n",
    "    practitioner_list = practitioner_df['text'].tolist()\n",
    "\n",
    "    appointment_type_df = pd.read_csv(os.path.join(entities_dir, \"appointment_type.csv\"))\n",
    "    appointment_type_list = appointment_type_df['text'].tolist()\n",
    "    \n",
    "    # Load the pre-generated appointment IDs\n",
    "    appointment_id_df = pd.read_csv(os.path.join(entities_dir, \"appointment_id.csv\"))\n",
    "    appointment_id_list = appointment_id_df['text'].tolist()\n",
    "    \n",
    "    print(f\"Loaded {len(practitioner_list)} practitioners.\")\n",
    "    print(f\"Loaded {len(appointment_type_list)} appointment types.\")\n",
    "    print(f\"Loaded {len(appointment_id_list)} pre-generated appointment IDs.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading entity CSVs: {e}\")\n",
    "    print(\"Please ensure you have run id_generator.py and all entity files are in place.\")\n",
    "\n",
    "\n",
    "# --- Load Intent Template DataFrames ---\n",
    "intent_dfs = {}\n",
    "intent_files = [\n",
    "    \"schedule\", \"reschedule\", \"cancel\", \"query_avail\",\n",
    "    \"greeting\", \"bye\", \"positive_reply\", \"negative_reply\", \"oos\"\n",
    "]\n",
    "\n",
    "for intent_name in intent_files:\n",
    "    try:\n",
    "        path = os.path.join(intents_dir, f\"{intent_name}.csv\")\n",
    "        intent_dfs[intent_name] = pd.read_csv(path)\n",
    "        print(f\"Loaded '{intent_name}.csv' with {len(intent_dfs[intent_name])} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find '{intent_name}.csv'. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac207476",
   "metadata": {},
   "source": [
    "## Process Intents and Inject Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6164c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2100 entries from simple intents.\n",
      "Processing complete. Total entries in dataset: 2664.\n"
     ]
    }
   ],
   "source": [
    "final_dataset = []\n",
    "\n",
    "# --- Process Simple Intents ---\n",
    "simple_intents = [\"greeting\", \"bye\", \"positive_reply\", \"negative_reply\", \"oos\"]\n",
    "for intent_name in simple_intents:\n",
    "    if intent_name in intent_dfs:\n",
    "        for index, row in intent_dfs[intent_name].iterrows():\n",
    "            final_dataset.append({\"text\": row['text'], \"intent\": row['intent'], \"entities\": []})\n",
    "print(f\"Processed {len(final_dataset)} entries from simple intents.\")\n",
    "\n",
    "# --- Process Complex Intents ---\n",
    "complex_intents = [\"schedule\", \"reschedule\", \"cancel\", \"query_avail\"]\n",
    "\n",
    "# The entity map now includes appointment_id from the start\n",
    "entity_map = {\n",
    "    'practitioner_name': practitioner_list,\n",
    "    'appointment_type': appointment_type_list,\n",
    "    'appointment_id': appointment_id_list\n",
    "}\n",
    "\n",
    "for intent_name in complex_intents:\n",
    "    if intent_name in intent_dfs:\n",
    "        for index, row in intent_dfs[intent_name].iterrows():\n",
    "            template = row['text']\n",
    "            injected_text = template\n",
    "            entities = []\n",
    "            placeholders = re.findall(r\"\\{(.+?)\\}\", template)\n",
    "            \n",
    "            for placeholder in placeholders:\n",
    "                # The logic is now generalized for all entities\n",
    "                if placeholder in entity_map:\n",
    "                    value_to_inject = random.choice(entity_map[placeholder])\n",
    "                    start_index = injected_text.find(\"{\" + placeholder + \"}\")\n",
    "                    if start_index != -1:\n",
    "                        injected_text = injected_text.replace(\"{\" + placeholder + \"}\", value_to_inject, 1)\n",
    "                        end_index = start_index + len(value_to_inject)\n",
    "                        entities.append({\"start\": start_index, \"end\": end_index, \"label\": placeholder})\n",
    "                else:\n",
    "                    print(f\"Warning: Found unknown placeholder '{placeholder}'\")\n",
    "\n",
    "            final_dataset.append({\n",
    "                \"text\": injected_text,\n",
    "                \"intent\": row['intent'],\n",
    "                \"entities\": sorted(entities, key=lambda e: e['start'])\n",
    "            })\n",
    "print(f\"Processing complete. Total entries in dataset: {len(final_dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6229d",
   "metadata": {},
   "source": [
    "## Shuffle and Save the final JSONL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b96d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shuffled.\n",
      "✅ Success! Dataset correctly saved.\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(final_dataset)\n",
    "print(\"Dataset shuffled.\")\n",
    "\n",
    "output_path_jsonl = os.path.join(raw_data_dir, \"dataset.jsonl\")\n",
    "with open(output_path_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for entry in final_dataset:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✅ Success! Dataset correctly saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedulebot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
