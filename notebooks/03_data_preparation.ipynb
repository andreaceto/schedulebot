{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbf7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value, Sequence\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b45037",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5f072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the tokenizer match the model that will be fine-tuned.\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b245506",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "This cell loads the generated `train.jsonl`, `validation.jsonl`, and `test.jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36f1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'intent', 'entities'],\n",
      "        num_rows: 1864\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'intent', 'entities'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'intent', 'entities'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "\n",
      "Example from training set:\n",
      "{'text': 'I must cancel my sabbatical leave meeting with Wright.', 'intent': 'cancel', 'entities': [{'start': 17, 'end': 41, 'label': 'appointment_type'}, {'start': 47, 'end': 53, 'label': 'practitioner_name'}]}\n"
     ]
    }
   ],
   "source": [
    "project_root = str(Path().cwd().resolve().parent)\n",
    "dataset_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(dataset_dir, \"train.jsonl\"),\n",
    "    \"validation\": os.path.join(dataset_dir, \"validation.jsonl\"),\n",
    "    \"test\": os.path.join(dataset_dir, \"test.jsonl\")\n",
    "}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "print(raw_datasets)\n",
    "print(\"\\nExample from training set:\")\n",
    "print(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543711e6",
   "metadata": {},
   "source": [
    "## Create Label Mappings\n",
    "Now, we create the mappings from string labels (e.g., \"schedule\", \"practitioner_name\") to integer IDs. This is essential for training. We also need to create tags for the BIO (Beginning, Inside, Outside) entity scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c3ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent mapping (intent2id): {'bye': 0, 'cancel': 1, 'greeting': 2, 'negative_reply': 3, 'oos': 4, 'positive_reply': 5, 'query_avail': 6, 'reschedule': 7, 'schedule': 8}\n",
      "\n",
      "NER mapping (ner2id): {'O': 0, 'B-appointment_id': 1, 'I-appointment_id': 2, 'B-appointment_type': 3, 'I-appointment_type': 4, 'B-practitioner_name': 5, 'I-practitioner_name': 6}\n"
     ]
    }
   ],
   "source": [
    "# --- Create Intent Label Mappings ---\n",
    "# Get all unique intent labels from the training data\n",
    "intent_labels = raw_datasets['train'].unique('intent')\n",
    "intent_labels.sort() # Sort for consistency\n",
    "id2intent = {i: label for i, label in enumerate(intent_labels)}\n",
    "intent2id = {label: i for i, label in enumerate(intent_labels)}\n",
    "print(f\"Intent mapping (intent2id): {intent2id}\\n\")\n",
    "\n",
    "\n",
    "# --- Create Entity (NER) Label Mappings in BIO format ---\n",
    "# Get all unique entity labels\n",
    "entity_labels = [\"appointment_id\", \"appointment_type\", \"practitioner_name\"]\n",
    "# Create the full list of BIO tags\n",
    "ner_tags = [\"O\"] # 'O' for tokens outside any entity\n",
    "for label in entity_labels:\n",
    "    ner_tags.append(f\"B-{label}\") # 'B' for Beginning of an entity\n",
    "    ner_tags.append(f\"I-{label}\") # 'I' for Inside of an entity\n",
    "\n",
    "id2ner = {i: label for i, label in enumerate(ner_tags)}\n",
    "ner2id = {label: i for i, label in enumerate(ner_tags)}\n",
    "print(f\"NER mapping (ner2id): {ner2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d2d71",
   "metadata": {},
   "source": [
    "## Preprocessing function\n",
    "This is the core function. It takes a single data example and does two things:\n",
    "1. Tokenizes the text.\n",
    "2. Aligns character-based entity spans (`start`, `end`) with the new wordpiece tokens, assigning the correct BIO tag ID to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f11c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # --- Intent Processing ---\n",
    "    # Convert intent strings to integer IDs\n",
    "    intent_ids = [intent2id[intent] for intent in examples['intent']]\n",
    "\n",
    "    # --- Tokenization ---\n",
    "    # Tokenize the text. `truncation=True` and `padding` are handled by the Trainer later.\n",
    "    tokenized_inputs = tokenizer(examples['text'], truncation=True, is_split_into_words=False, return_offsets_mapping=True)\n",
    "\n",
    "    # --- Entity (NER) Label Alignment ---\n",
    "    ner_labels = []\n",
    "    for i, entities in enumerate(examples['entities']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        # Start with all tokens labeled as 'O' (Outside)\n",
    "        label_ids = [ner2id[\"O\"]] * len(word_ids)\n",
    "        \n",
    "        # For each entity, find the corresponding tokens and assign B- and I- tags\n",
    "        for entity in entities:\n",
    "            start_char, end_char, label = entity['start'], entity['end'], entity['label']\n",
    "            \n",
    "            for j, word_id in enumerate(word_ids):\n",
    "                if word_id is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get the character span for the current token\n",
    "                token_char_span = tokenized_inputs['offset_mapping'][i][j]\n",
    "                if token_char_span is None:\n",
    "                    continue\n",
    "\n",
    "                token_start, token_end = token_char_span\n",
    "                \n",
    "                # Check if the token is part of the entity\n",
    "                if start_char < token_end and end_char > token_start:\n",
    "                    if label_ids[j] == ner2id[\"O\"]:\n",
    "                        # Assign the 'B-' tag to the first token\n",
    "                        label_ids[j] = ner2id[f\"B-{label}\"]\n",
    "                    else:\n",
    "                        # Assign the 'I-' tag to subsequent tokens within the same entity\n",
    "                        label_ids[j] = ner2id[f\"I-{label}\"]\n",
    "\n",
    "        ner_labels.append(label_ids)\n",
    "\n",
    "    # Add the final processed labels to our tokenized inputs\n",
    "    tokenized_inputs[\"intent_label\"] = intent_ids\n",
    "    tokenized_inputs[\"ner_labels\"] = ner_labels\n",
    "    \n",
    "\t# Remove offset_mapping\n",
    "    tokenized_inputs.pop(\"offset_mapping\", None)\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2614d4f",
   "metadata": {},
   "source": [
    "## Apply Preprocessing and Save\n",
    "Now we apply this function to our entire dataset and save the final, processed version. This is what we will load directly in the fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8480f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab8b54f84e04901ae272cdd0d129b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d09c0bd59e439e9a4e40b3a48cbaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b01d1bf51f40bf99206af559b46f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670527ab653d46dc8a53f5629d91ce01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6744f03db8404eb9a9b3de65c202b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed dataset saved successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'intent_label', 'ner_labels'],\n",
      "        num_rows: 1864\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'intent_label', 'ner_labels'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'intent_label', 'ner_labels'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to all splits of the dataset\n",
    "processed_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets['train'].column_names)\n",
    "\n",
    "# Define the features for our processed dataset, including the new ClassLabels\n",
    "features = Features({\n",
    "    'input_ids': Sequence(Value('int64')),\n",
    "    'attention_mask': Sequence(Value('int8')),\n",
    "    'intent_label': ClassLabel(names=list(intent2id.keys())),\n",
    "    'ner_labels': Sequence(ClassLabel(names=list(ner2id.keys())))\n",
    "})\n",
    "\n",
    "# Cast the processed datasets to the defined features to include the label names\n",
    "processed_datasets = processed_datasets.cast(features)\n",
    "\n",
    "# Save the processed dataset locally\n",
    "output_dir = os.path.join(project_root, \"data\", \"processed\", \"hasd_processed\")\n",
    "processed_datasets.save_to_disk(output_dir)\n",
    "\n",
    "print(\"\\nProcessed dataset saved successfully!\")\n",
    "print(processed_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedulebot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
